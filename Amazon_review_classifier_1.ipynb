{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47e5356e",
    "outputId": "5a260d8a-2b49-41d0-8e3b-725b32705a02"
   },
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414,
     "referenced_widgets": [
      "56de452d48494c62a8ccac353efd4186",
      "ace64df216984f86b36554193fd2a8cc",
      "2e592d275ea04c8387b9a4206a3b000e",
      "fa5c3643c105474f9c549790904beb5a",
      "54f03531d8a74e38b8c4c972ecf6cea5",
      "f2ad4c1ecbef4cbb88f44a2409821d5d",
      "3e1803b925c7426bbd69e8e1fd08321b",
      "25711b3808af441c94d9f5c23da56bbd",
      "3db27378a2e64a93b8031075ebc9fda2",
      "508a72eb6e8f4ff395d7ed2f24c2523f",
      "d3a7283623734107b0bc5ec053e981b5",
      "cb3c251919d9490bb6e79550b66ec0e9",
      "0c335b56ded84cd897575e85b3ba4788",
      "fbdaa8da8bee427fa43c672a4e65bc36",
      "1605bd98c59b4a4e9a457b16be5a5858",
      "9b64043f7ef7480b802987dba5e65127",
      "e0f40f9473ff40008c0677619d71a457",
      "0f3baaf750eb48698710d41273cc7891",
      "09aed470905545e4bc5edfaa1a601b7b",
      "0bdfaa3cd48b4569bf16456b73842206",
      "bad9a09fe7cb4f5c973625f25a08d1ab",
      "437ced9c4aec4e359cb51afac16aafe5",
      "17d7fa5637bc4ed2be3aa5031e7fd8af",
      "ec4aebe5f3fd4b20b51faad31ecedbff",
      "0dea38dc774a49ff9dc15fb034c346bd",
      "594c9e95cd3d482fb7a745d16020481c",
      "f97e91641b464e0f9b403553b7466f23",
      "759b02ee74044cf8b5be52da9ce0485a",
      "0e570454efa14892ade214c443941570",
      "737e4c6b858b4182a7101e11db232d56",
      "0f8f755acae64add9ce775581f6364fa",
      "04fe98f880be49c78c8c386b703b556b",
      "4b3acfc80552429d82bb6f4bf5219a6d",
      "b3937239aa9549c49de406433d6e0555",
      "5f683c744f4c4720918e84d524ceb7a5",
      "034fedcae26c4f13a38b8f1c18850b8a",
      "c39f367acfa04fa5bdea8b728da373a4",
      "73f9ec514c404c66b2421ca1cf9fddc2",
      "f1e52cf2a40643f88da5b62deff84aac",
      "5da5583bf97240a2991314522480919d",
      "58e93c0484354b6c957bec0ebf118bbf",
      "279d7d5a0e7349d3b781e3c4ca0c1a20",
      "8bacfc8fb59c4433bb01bc3202694457",
      "ac3c3bb9e8a44b999cb37ef8cf90ccc1",
      "954eb07dc0734d2b9f6537227cc29edd",
      "5419cf4860904f14aa646dbb426e4908",
      "8c039f6af057422082721619dd109d32",
      "9b733f4860304da6b2b0ff8cc0b25835",
      "7da38b73cda1478eb6b7056c8c81efbb",
      "9319e304e9894f6488856cf6a13d7bf5",
      "a064147c89d740d09294b6abf9b3ed13",
      "333eba0c439f43b7b16728b717ba748f",
      "fba8eaac1aee4491b7ba0bc922995eb4",
      "8b370d9fa2c045af9836c7d2f01270dc",
      "bcd317fad5964a25829d603395969a34",
      "ca83b794c33445f5a5d622777b9b327f",
      "baf4efb84a35477283432756f54aa983",
      "550fd6de97eb4617a196e53227c8b3bc",
      "c234ded11efc4cffb6ed420ef57ac551",
      "c944cad809de4aaabf6e64110ef14913",
      "1b8c653fbd184039abc4e0011e94e05e",
      "7f3c82797cf74cb6a6ddd08fa4bffa2d",
      "a2286b2331394f07b8048b26dc9c656b",
      "dc83e794b04f48a08375a8df89100ebb",
      "45c27274a9c54b97a9ebfc5db6d97c58",
      "bbda323c1c1240b797384b1ee4bd9ca4",
      "c2568675e70545a0bbfab43f1a5b0c36",
      "558304fc93884062bed2c14741e7adaa",
      "cb21e7530497431181fdcb256e750217",
      "4560b7114b344fef8534e041fc452209",
      "6b9da9ee833741df8a4cecedb7421926",
      "81fc102e17d94df79b04b6688f4a3745",
      "d01c0e79b53340ec8c7c48b8f70d4fb3",
      "dd1a072840e849d793e5fa66b2be3516",
      "3dddb8559c7641ddb509ecf184ef95a0",
      "37ba30cb0b774f56920f5f4628170634",
      "fb70b07610924f1ea96a471745ed3090",
      "55a2fffe2b6344a1beda719d58636320",
      "c68e15fb998d46a09b9e61157eed6b36",
      "b76947885e3d4b8ca9122ead515804bf",
      "74dc878ca8ef4055a1d53426dba2ed8b",
      "911b94ddb32e4c119c700b258f2f5f84",
      "db1580c15e224421961417497d05df43",
      "ff5d5a4c6509491db23feb2d83bd3777",
      "b9bec1d4b85043fe85fcfab96feb9a67",
      "f2685855da364a54bffb8ce14aeb1afa",
      "f80f075a51b741669f7f132753f67ae0",
      "a4eef89a64c44f0193b8e05b0bd806e5"
     ]
    },
    "id": "33e49646",
    "outputId": "acddac00-52cf-4eea-9555-d5f056ca2329"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"amazon_polarity\", split=\"train\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "22a6ef6d",
    "outputId": "4d2a82df-ecc5-4563-df59-3e96985fb9fc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert only a small slice first (safe)\n",
    "df = pd.DataFrame(dataset[:10])   # change 1000 â†’ more later\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "cea65237",
    "outputId": "6108eb7b-f267-4b50-8148-b188e88937a3"
   },
   "outputs": [],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "id": "49dbcd8f"
   },
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# with open(\"amazon_polarity_train.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerow([\"label\", \"title\", \"content\"])\n",
    "\n",
    "#     for item in dataset:\n",
    "#         writer.writerow([item[\"label\"], item[\"title\"], item[\"content\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "349b6d7f",
    "outputId": "b054d6b1-1df4-4f3f-d5ac-a958622b69c2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"amazon_polarity_train.csv\",\n",
    "    engine=\"python\",      # more tolerant parser\n",
    "    on_bad_lines=\"skip\"   # skip broken rows\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LrLJsd6tr8sJ",
    "outputId": "d1513227-7e1d-4602-ae46-0f039bf45a91"
   },
   "outputs": [],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "16d4907c",
    "outputId": "41367ef1-47d9-4b5c-b7b3-20f77dfed954"
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ae81d20",
    "outputId": "4b59d995-70e7-4f3c-9a06-4dde293d4d69"
   },
   "outputs": [],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "65c32949",
    "outputId": "6e1b65da-1a77-4a7a-f145-a7918ed43420"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "ddb30fb9",
    "outputId": "0e0a3268-bb37-4bda-f7b0-14aac5795ecd"
   },
   "outputs": [],
   "source": [
    "(df.isnull().mean() * 100).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 770
    },
    "id": "4f1865e7",
    "outputId": "d9b18244-07d2-44f2-a5d8-aa5cbbe35de6"
   },
   "outputs": [],
   "source": [
    "df[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0833c81",
    "outputId": "366d6025-fc38-4917-9033-a4d1afb29b0d"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "02bab7c5",
    "outputId": "5d6aa3d0-0db2-443e-8aeb-000034ea273d"
   },
   "outputs": [],
   "source": [
    "df[df[\"title\"].isna()][\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "id": "9cc69caf"
   },
   "outputs": [],
   "source": [
    "df_filtered = df.dropna(subset=[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7b10780",
    "outputId": "f9c14c6c-75c3-4e35-d182-78dbe0960955"
   },
   "outputs": [],
   "source": [
    "print(\"Original rows:\", len(df))\n",
    "print(\"Filtered rows:\", len(df_filtered))\n",
    "print(\"Rows removed:\", len(df) - len(df_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "9991ee26",
    "outputId": "43771d31-f9a0-4684-f2b5-6436151d8840"
   },
   "outputs": [],
   "source": [
    "df_filtered.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "id": "9ac884bc"
   },
   "outputs": [],
   "source": [
    "df_filtered.to_csv(\n",
    "    \"ARC_Filtered.csv\",\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62b1c5d0",
    "outputId": "f5eef8f8-7097-4acf-cb65-fa250cc91cab"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ARC_Filtered.csv\")\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "dc9b68b2",
    "outputId": "4fc0fdbf-6756-482c-91eb-ad826c9bf052"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "40c56af7",
    "outputId": "0ef4cda3-1e61-467f-e7de-e1bdaee5c047"
   },
   "outputs": [],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "8478d19a",
    "outputId": "fcee4d9e-a60e-4c9c-e7b8-213658e45e3d"
   },
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"content\"]\n",
    "\n",
    "df[\"char_len\"] = df[\"text\"].str.len()\n",
    "df[\"word_len\"] = df[\"text\"].str.split().str.len()\n",
    "\n",
    "df[[\"char_len\", \"word_len\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "id": "23c6b0b2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "id": "7b8b96ef"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n",
    "            GELU(), ## Activation\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        # 2*4*768\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n",
    "        # 2*4*768\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "import torch\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "id": "da612355"
   },
   "outputs": [],
   "source": [
    "class AmazonPolarityDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer, max_length=128):\n",
    "        self.dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        text = item[\"title\"] + \" \" + item[\"content\"]\n",
    "\n",
    "        token_ids = self.tokenizer.encode(text)\n",
    "        token_ids = token_ids[:self.max_length]\n",
    "\n",
    "        # padding\n",
    "        if len(token_ids) < self.max_length:\n",
    "            token_ids += [0] * (self.max_length - len(token_ids))\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(token_ids, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(item[\"label\"], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "7f8afd319edd447688955a07716f476b",
      "8e6839c60b5e4ed882d616846e47777c",
      "398dbd013f554cc49beae7986a192465",
      "2233b779ccc24b359941660f51f6db34",
      "ffd3327f0a574466ad75309888bc7a73",
      "c0b28c7c3fb64c30950847d52e05ae7f",
      "eed63d7129ed467084f4fb9600311bd0",
      "6447a72e054346078aeb6d5639535aa4",
      "2f8f774519d54bf99c633832b5c8b699",
      "03ad1114823645249c8dbc94613ff89d",
      "8cd6febce6b646239b9a5bcc2c736aec",
      "1ddea626fa424171ba0327da9b0862d1",
      "98bb550d86ec4e69a8e6efe6c1ef2aad",
      "5676f95b1c92420e84ffc9e598f04945",
      "b1196de3767044359e55e2d839ce7ccb",
      "c2378887b87d4677aea499bddb5d96ea",
      "95176640126348ba9d78fbdb7b1f56ee",
      "3fbc559655344bc2949d868f54be4ce8",
      "cbe92405befd474fb9a84e9c1c7b6911",
      "a2fadae694d545de9ea36b1c962b043e",
      "898acbcf51704da68849acaf9f95958d",
      "f4cf9b3ecdc0487e91531403589ebd85",
      "93263b3090654c8c8cfee36257483e7e",
      "a042294725234c0095780e14a6bdceba",
      "5ef831d32d8b4e34bd6c4cf723409f47",
      "72a23baa7f3143dfbdd64cf6ddbf7e8d",
      "ec38b750f6b545139f989398ee92cba0",
      "e050689a3bd14900a517fcd8c1fe53ae",
      "f589d9a7cf2b425e8200a89c282542f2",
      "e3a21c2943f04df9ba5ae1a194bd7508",
      "495774d4109f4795864c4f702b1ec32a",
      "4426fde37cf34fa18fed11a72833df42",
      "8e186836b21741e4a5239edf279f1cfe",
      "a5c44e352a1b48d998418d7461d213d9",
      "51dad935099d4a16b029329250be65a6",
      "173c958cbbfa430cb77294256143faec",
      "77c1608e26294ad78559f040a52ac75a",
      "078545c6395f47ed90771cf42b9d417e",
      "a196c8ae560246ad81ddec8c6553a833",
      "12f353c481c34017b9243091e52370de",
      "abe6b16b2cdb4338ac5b9011165d6ab9",
      "255f992db877496d84f54b845adcfefe",
      "5507155a673040b083835e8dade73389",
      "16588512ccf544b49dd295ada0931cbc",
      "d2a271d07f2943b1bfe46b1350e7d613",
      "39dced548f38484eaf292ce167f12cdc",
      "de5001bf2d184b99968cc8a4104c60da",
      "dd4098e7077d44818e0a3fc05249be7f",
      "a2b3b2b76ace422cb443c6370780f0f4",
      "143e92e2321a44e5bba23d2038d87570",
      "1b2032b4519a4418bf653f45dc0e6aff",
      "30885e3b71c44411a613513864880cf6",
      "8d1fea0ba63144c490a90d3c7664a3d1",
      "7c95fbcddd1240f5b1bbe410735dd3c0",
      "7281ee372b754a9fa04a37ea27f9db2e"
     ]
    },
    "id": "2d2ecf49",
    "outputId": "0406f19a-5f7a-4f62-8505-a6078667e2aa"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "id": "jQFmWc_jNdsJ"
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cQdlWtPTNghP",
    "outputId": "2983adb0-c77b-45c9-a7d9-37507128401e"
   },
   "outputs": [],
   "source": [
    "print(tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3f2c289b",
    "outputId": "f29eb577-41db-47fb-fd86-3d382f6ff408"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "# take only 50k samples\n",
    "subset_size = 100000\n",
    "subset_indices = list(range(subset_size))\n",
    "\n",
    "train_subset = Subset(dataset, subset_indices)\n",
    "\n",
    "train_dataset = AmazonPolarityDataset(\n",
    "    train_subset,\n",
    "    tokenizer,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "id": "c3d594b9"
   },
   "outputs": [],
   "source": [
    "class GPTForClassification(nn.Module):\n",
    "    def __init__(self, gpt, num_classes, pad_token_id):\n",
    "        super().__init__()\n",
    "        self.gpt = gpt\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.classifier = nn.Linear(\n",
    "            gpt.out_head.in_features,  # emb_dim = 768\n",
    "            num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        seq_len = input_ids.size(1)\n",
    "\n",
    "        # ðŸš€ GPT runs in inference mode\n",
    "        with torch.no_grad():\n",
    "            tok_embeds = self.gpt.tok_emb(input_ids)\n",
    "            pos_embeds = self.gpt.pos_emb(\n",
    "                torch.arange(seq_len, device=input_ids.device)\n",
    "            )\n",
    "\n",
    "            x = tok_embeds + pos_embeds\n",
    "            x = self.gpt.drop_emb(x)\n",
    "            x = self.gpt.trf_blocks(x)\n",
    "            x = self.gpt.final_norm(x)\n",
    "\n",
    "            # x = x[:, -1, :]   # last token pooling\n",
    "            attention_mask = input_ids.ne(self.pad_token_id)   # BoolTensor\n",
    "            last_token_idx = attention_mask.long().sum(dim=1) - 1\n",
    "\n",
    "            x = x[torch.arange(x.size(0), device=x.device), last_token_idx]\n",
    "\n",
    "        logits = self.classifier(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "id": "c179de15"
   },
   "outputs": [],
   "source": [
    "clf_model = GPTForClassification(\n",
    "    model,\n",
    "    num_classes=2,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "id": "1y4qZAu5NyWz"
   },
   "outputs": [],
   "source": [
    "for p in clf_model.gpt.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tvs4Sx7fN0zr",
    "outputId": "33ceae91-b52d-43d0-887d-65616d86f7e1"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "clf_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UwezQ5IXuGXv",
    "outputId": "2b96146a-f324-4b89-de61-23dee5f8f7f2"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(type(batch))\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "id": "iaoX2HB1vWhF"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "id": "uYD0ifSmvZnx"
   },
   "outputs": [],
   "source": [
    "clf_model = clf_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "id": "293c28cf"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "input_ids = batch[\"input_ids\"].to(device)\n",
    "labels = batch[\"labels\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zYhVJ5nRvgXd",
    "outputId": "3f28bc1c-9196-423f-cb2f-c2dfc3d9a36c"
   },
   "outputs": [],
   "source": [
    "%time logits = clf_model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "id": "e8d14769"
   },
   "outputs": [],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# clf_model.to(device)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.AdamW(\n",
    "#     clf_model.classifier.parameters(),\n",
    "#     lr=2e-4\n",
    "# )\n",
    "\n",
    "# clf_model.train()\n",
    "\n",
    "# for epoch in range(3):\n",
    "#     total_loss = 0\n",
    "\n",
    "#     for step, batch in enumerate(train_loader):\n",
    "#         input_ids = batch[\"input_ids\"].to(device)\n",
    "#         labels = batch[\"labels\"].to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         logits = clf_model(input_ids)\n",
    "#         loss = criterion(logits, labels)\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         if step % 10 == 0:\n",
    "#             print(f\"Epoch {epoch+1} | Step {step} | Loss {loss.item():.4f}\")\n",
    "\n",
    "#     print(f\"Epoch {epoch+1} | Avg Loss {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "id": "ad6968b0"
   },
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, linear_layer, r=8, alpha=16):\n",
    "        super().__init__()\n",
    "        self.linear = linear_layer\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / r\n",
    "\n",
    "        in_dim = linear_layer.in_features\n",
    "        out_dim = linear_layer.out_features\n",
    "\n",
    "        self.lora_A = nn.Parameter(torch.randn(in_dim, r) * 0.01)\n",
    "        self.lora_B = nn.Parameter(torch.zeros(r, out_dim))\n",
    "\n",
    "        # freeze original weights\n",
    "        for p in self.linear.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + (x @ self.lora_A @ self.lora_B) * self.scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "id": "cC6ZjzrbcHLh"
   },
   "outputs": [],
   "source": [
    "def apply_lora_to_gpt(gpt, r=8, alpha=16):\n",
    "    for block in gpt.trf_blocks:\n",
    "        block.att.W_query = LoRALinear(block.att.W_query, r, alpha)\n",
    "        block.att.W_key   = LoRALinear(block.att.W_key, r, alpha)\n",
    "        block.att.W_value = LoRALinear(block.att.W_value, r, alpha)\n",
    "\n",
    "apply_lora_to_gpt(model, r=8, alpha=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {
    "id": "VKxdCyq9Y8HE"
   },
   "outputs": [],
   "source": [
    "pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CLIhi9CtZAvl",
    "outputId": "fbeecbea-b938-4258-9336-949e77ef2e06"
   },
   "outputs": [],
   "source": [
    "print(pad_token_id)   # should be an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {
    "id": "EPaI82n9cKIB"
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "clf_model = GPTForClassification(model, num_classes, pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "id": "32Ivpb0hcMJt"
   },
   "outputs": [],
   "source": [
    "for p in clf_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "for name, p in clf_model.named_parameters():\n",
    "    if \"lora_\" in name or \"classifier\" in name:\n",
    "        p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_kGVYSA7cPzI",
    "outputId": "5d6845e8-22fc-4033-9668-159459fd3349"
   },
   "outputs": [],
   "source": [
    "sum(p.numel() for p in clf_model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ix2p_aS5IfK",
    "outputId": "6b056d43-98df-469b-aa09-97a8b14fc592"
   },
   "outputs": [],
   "source": [
    "print(\"Trainable parameters:\")\n",
    "for name, p in clf_model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(name, p.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2L04sW-8cR2h",
    "outputId": "89eb570d-5e12-47dc-ad5c-0456681f8a37"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clf_model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, clf_model.parameters()),\n",
    "    lr=3e-4\n",
    ")\n",
    "\n",
    "clf_model.train()\n",
    "\n",
    "for epoch in range(2):   # keep short\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = clf_model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1} | Step {step} | Loss {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Avg Loss {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oTLqAuEyFobs",
    "outputId": "8b83a930-e913-4575-9f0c-5dcdf4e13d94"
   },
   "outputs": [],
   "source": [
    "# Save model weights\n",
    "torch.save(clf_model.state_dict(), \"sentiment_model.pt\")\n",
    "\n",
    "# (Optional but recommended) Save tokenizer too\n",
    "tokenizer.save_pretrained(\"tokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {
    "id": "3WQ2vP8yFryd"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "clf_model = GPTForClassification(\n",
    "    gpt=model,                     # same GPT used during training\n",
    "    num_classes=2,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "clf_model.load_state_dict(\n",
    "    torch.load(\"sentiment_model.pt\", map_location=device)\n",
    ")\n",
    "\n",
    "clf_model.to(device)\n",
    "clf_model.eval()\n",
    "\n",
    "# Load tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {
    "id": "OK6kjJcRcXkB"
   },
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    # clf_model.eval()\n",
    "\n",
    "    enc = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "    input_ids = enc[\"input_ids\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = clf_model(input_ids)\n",
    "        pred = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    return \"Positive\" if pred == 1 else \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "Jnn5K_s5cgO3",
    "outputId": "888866ef-ac4b-426a-ec85-2c03d002418b"
   },
   "outputs": [],
   "source": [
    "predict(\"This product is absolutely worst.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "macrvn-lcigV",
    "outputId": "936e0377-c18f-4a74-ea6d-40219072ee83"
   },
   "outputs": [],
   "source": [
    "predict(\"This product is absolutely fantastic. I loved it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "P6PDN7MGwqc_",
    "outputId": "51121918-326b-4420-a149-67a6f7945e1d"
   },
   "outputs": [],
   "source": [
    "predict(\"Worst purchase ever. Complete waste of money.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "fqRW_zEPwuPR",
    "outputId": "e2fe209f-6847-45eb-bf2d-e54468bf8e99"
   },
   "outputs": [],
   "source": [
    "predict(\"Bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "Wx38TeVfwzHe",
    "outputId": "69871ca5-cdd1-4ecf-b423-7547e8799cd5"
   },
   "outputs": [],
   "source": [
    "predict(\"Cheap plastic, feels like it will snap.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "mrcsiVVWZsS0",
    "outputId": "5344a109-6456-4f1e-e38c-b4ef20c552b9"
   },
   "outputs": [],
   "source": [
    "predict(\"Doesn't do what the listing claims.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "vA0K_LSwhLJl",
    "outputId": "a3caf7aa-b05d-4e4d-c519-0760df61f325"
   },
   "outputs": [],
   "source": [
    "predict(\"Arrived crushed packaging.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "b06ziXd8hW65",
    "outputId": "2894d766-cd0f-4b97-c7ca-cc272e1813cb"
   },
   "outputs": [],
   "source": [
    "predict(\"Arrived damaged packaging.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "sTrxZQZqhpcv",
    "outputId": "7cd64286-1214-4776-b42e-cb2061408f91"
   },
   "outputs": [],
   "source": [
    "predict(\"Seller unresponsive to issues.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "vYlaAV5Th8yN",
    "outputId": "cf495147-467a-4b0e-c63d-6ef6ad75ba7c"
   },
   "outputs": [],
   "source": [
    "predict(\"Received the wrong size.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "QmZD74rbiGSZ",
    "outputId": "ea5568a5-abce-4d6f-ddc1-246b9b464387"
   },
   "outputs": [],
   "source": [
    "predict(\"Received the wrong item.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "wRkgh8pQiRe5",
    "outputId": "95a531e1-a789-4784-d382-df8cc70935bc"
   },
   "outputs": [],
   "source": [
    "predict(\"Received the wrong color.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "JX2DMe-diUyd",
    "outputId": "31483339-b9e9-43f5-d9de-0210e2e42387"
   },
   "outputs": [],
   "source": [
    "predict(\"Received the wrong item/color/size.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {
    "id": "JI-JKPF3iW30"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
